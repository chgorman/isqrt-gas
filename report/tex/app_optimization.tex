\section{Additional Algorithms and Further Optimizations}
\label{app:optimization}

\subsection{Additional Algorithms}
\label{app:opt_additiona_algorithms}

We discuss potential lines of algorithmic improvement;
see the discussion in Appendix~\ref{app:variants_unrolled3}
about a variant of \UnrolledThree{}.

\subsubsection{Polynomial Approximations}
\label{app:opt_polynomial}

The results of \Linear{} in Table~\ref{table:gas_costs_3}
shows that a linear approximation provides
a good initial approximation.
It may be possible to extend this linear (affine) approximation
to higher degree polynomials;
however, the author thinks that will not be productive.
This follows from the fact that a more complex initialization
algorithm will require additional gas
and Newton iterations are pretty cheap (48 gas).
Almost 8 bits of accuracy is needed to remove \emph{one} Newton iteration;
the author does not think such an accurate approximation is
possible with only 48 gas.

\subsubsection{Rational Approximations}
\label{app:opt_rational}

The results \HyperFour{} in Table~\ref{table:gas_costs_3}
shows that hyperbolas provide a good initial approximation;
however, \HyperFour{} is not more efficient than \UnrolledThree{}
(or \Linear{}).
It seems unlikely that extending this method
(to higher degree rational functions)
would produce a more efficient result:
as mentioned in Appendix~\ref{app:opt_polynomial},
Newton iterations are cheap and good approximations are expensive.

\subsubsection{Lookup Tables}
\label{app:opt_lookup}

The results in Table~\ref{table:gas_costs_3}
show \LookupFour{} and \LookupEight{} produce decent results;
however, the gas costs are essentially the same even though \LookupEight{}
uses one less Newton iteration than \LookupFour{}.
This leads us to expect that higher precision
lookup tables will not result in lower gas costs.

In~\cite{FormalVerIsqrt}, the method for computing
square roots is actually based around computing \emph{inverse square roots}.
We note, though, that the setting and constraints are different
as~\cite{FormalVerIsqrt} focuses on the inverse square root algorithm
in order to not have to perform division.
Within the Ethereum Virtual Machine,
the cost of integer multiplication and division are the same;
thus, although it would be interesting to implement the algorithm,
the author does not expect it to produce an efficient algorithm
(that is, better than \UnrolledThree{} or \Linear{}).

\subsection{Higher-Order Approximations}
\label{app:higher-order}

We have primarily focused on variations of Newton's method.
We now briefly look at higher-order methods.
In particular, we look at Halley's method
for computing square roots~\cite[Section 2]{guo2010newton}.
We mentioned that Newton's method has \emph{quadratic convergence}:
correct digits doubling approximately every iteration.
Halley's method has \emph{cubic convergence}:
correct digits \emph{tripling} approximately every iteration.

Although the increase in the rate of convergence is desirable,
from the discussion which follows it appears that this method
(and others like it)
will not give us a more efficient algorithm due to the increased complexity
of the calculation.

When solving $x^{2} - \alpha = 0$ (that is, computing square roots),
Newton's method reduces to

\begin{equation}
    x_{n+1} = \frac{1}{2}\parens{x_{n} + \frac{\alpha}{x_{n}}}.
\end{equation}

\noindent
When solving $x^{2} - \alpha = 0$ using Halley's method,
we have the recursive formula

\begin{equation}
    x_{n+1} = \frac{x_{n}^{3} + 3\alpha x_{n}}{3x_{n}^{2} + \alpha}.
    \label{eq:halley_iteration}
\end{equation}

\noindent
If we set $\eps = t - \sqrt{\alpha}$ and

\begin{equation}
    h_{\alpha}(t) = \frac{t^{3} + 3\alpha t}{3t^{2} + \alpha},
\end{equation}

\noindent
then we see

\begin{equation}
    h_{\alpha}(t) - \sqrt{\alpha}= \frac{\eps^{3}}{3t^{2} + \alpha}.
\end{equation}

\noindent
The ``$\eps^{3}$'' shows the cubic convergence of the algorithm.

When attempting to perform this operation within Solidity,
the first problem we notice in Eq.~\eqref{eq:halley_iteration}
is that both the numerator and denominator
will overflow when $\alpha$ is large:
$\alpha \ge 2^{180}$ for the numerator; 
$\alpha \ge 2^{255}$ for the denominator.
While this could be handled,
that additional cost alone renders this algorithm impractical.
Thus, these methods do not appear relevant.
An optimized form of \texttt{mulDiv}
($\floor{(a\cdot b)/c}$ with no overflow)
costs 550 gas per call~\cite{MathInSolidity3}.

\subsection{Detailed Gas Cost}

To better see where gas is being spent,
we include a detailed analysis of the gas costs associated
with computing

\begin{align}
    \floor{\sqrt{2^{255}}} &= 240615969168004511545033772477625056927
        \nonumber\\
    &= \texttt{0xb504f333f9de6484597d89b3754abe9f}.
\end{align}

\noindent
We let $x = 2^{255}$ throughout to simplify our discussion.

\subsubsection{Initialization}

The main part of the initialization $x$ is to determine
the correct order-of-magnitude.
In particular, we want to compute $e$ so that

\begin{equation}
    2^{e-1} \le \sqrt{x} < 2^{e}.
\end{equation}

\noindent
We note that this is done implicitly in most instances.
Computing this estimate for $x$ costs 426 gas;
it is unclear if this could be reduced by any significant amount.

\subsubsection{Newton Iterations}

The cost of each unrolled Newton iteration costs 48 gas.
A Newton iteration inside a \texttt{while} loop
is estimated to be 90 gas from~\ref{app:deterministic_detailed}.
Thus, using a fixed number of Newton iterations is more efficient
in general.

\subsubsection{Final Check}

A final check ensures a valid result is returned
(checking if $r^{2} \le x$) is estimated to cost 29 gas,
and there does not appear to be a way to make this less expensive.
The return logic will be discussed in Appendix~\ref{app:final_check}.


\subsection{Potential Optimizations}

Based on the cost of unrolled Newton iterations (48 gas per unrolled iteration
and 90 gas within a \texttt{while} loop),
reducing the number of Newton iterations only reduces the overall
gas cost so much.

\paragraph{Additional Approximations}
As mentioned in Appendix~\ref{app:opt_additiona_algorithms},
it is expected that polynomial estimates, rational estimates,
and lookup tables will not lead to an overall reduction in gas.
It is unclear where additional approximation methods would come from.
The best algorithms presented here
(\UnrolledThree{} (Listing~\ref{list:newton-unrolled-3})
followed closely by \Linear{} (Listing~\ref{list:newton-linear}))
produce 2 or 4 bits of initial precision.

\paragraph{Assembly}
One way to reduce gas is to write all algorithms in assembly.
This is not completely separate from algorithm development,
as it may be the case that optimized (assembly) versions
of algorithms have a different ordering.
This requires a thorough knowledge of Solidity assembly
and the result should be audited to ensure the algorithm
is properly implemented.
